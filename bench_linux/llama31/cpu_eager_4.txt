
python3 torchchat.py generate llama3.1 --quantize '{"linear:int4": {"groupsize": 256}, "precision": {"dtype":"bfloat16"}, "executor":{"accelerator":"cpu"}}' --prompt "Once upon a time," --max-new-tokens 256 --num-samples 3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
python3 torchchat.py generate llama3.1 --quantize '{"linear:int4": {"groupsize": 256}, "precision": {"dtype":"bfloat16"}, "executor":{"accelerator":"cpu"}}' --prompt "Once upon a time," --max-new-tokens 256 --num-samples 3
Note: NumExpr detected 22 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
NumExpr defaulting to 16 threads.
PyTorch version 2.5.0.dev20240728+cu121 available.
linear: layers.0.attention.wq, in=4096, out=4096
linear: layers.0.attention.wk, in=4096, out=1024
linear: layers.0.attention.wv, in=4096, out=1024
linear: layers.0.attention.wo, in=4096, out=4096
linear: layers.0.feed_forward.w1, in=4096, out=14336
linear: layers.0.feed_forward.w2, in=14336, out=4096
linear: layers.0.feed_forward.w3, in=4096, out=14336
linear: layers.1.attention.wq, in=4096, out=4096
linear: layers.1.attention.wk, in=4096, out=1024
linear: layers.1.attention.wv, in=4096, out=1024
linear: layers.1.attention.wo, in=4096, out=4096
linear: layers.1.feed_forward.w1, in=4096, out=14336
linear: layers.1.feed_forward.w2, in=14336, out=4096
linear: layers.1.feed_forward.w3, in=4096, out=14336
linear: layers.2.attention.wq, in=4096, out=4096
linear: layers.2.attention.wk, in=4096, out=1024
linear: layers.2.attention.wv, in=4096, out=1024
linear: layers.2.attention.wo, in=4096, out=4096
linear: layers.2.feed_forward.w1, in=4096, out=14336
linear: layers.2.feed_forward.w2, in=14336, out=4096
linear: layers.2.feed_forward.w3, in=4096, out=14336
linear: layers.3.attention.wq, in=4096, out=4096
linear: layers.3.attention.wk, in=4096, out=1024
linear: layers.3.attention.wv, in=4096, out=1024
linear: layers.3.attention.wo, in=4096, out=4096
linear: layers.3.feed_forward.w1, in=4096, out=14336
linear: layers.3.feed_forward.w2, in=14336, out=4096
linear: layers.3.feed_forward.w3, in=4096, out=14336
linear: layers.4.attention.wq, in=4096, out=4096
linear: layers.4.attention.wk, in=4096, out=1024
linear: layers.4.attention.wv, in=4096, out=1024
linear: layers.4.attention.wo, in=4096, out=4096
linear: layers.4.feed_forward.w1, in=4096, out=14336
linear: layers.4.feed_forward.w2, in=14336, out=4096
linear: layers.4.feed_forward.w3, in=4096, out=14336
linear: layers.5.attention.wq, in=4096, out=4096
linear: layers.5.attention.wk, in=4096, out=1024
linear: layers.5.attention.wv, in=4096, out=1024
linear: layers.5.attention.wo, in=4096, out=4096
linear: layers.5.feed_forward.w1, in=4096, out=14336
linear: layers.5.feed_forward.w2, in=14336, out=4096
linear: layers.5.feed_forward.w3, in=4096, out=14336
linear: layers.6.attention.wq, in=4096, out=4096
linear: layers.6.attention.wk, in=4096, out=1024
linear: layers.6.attention.wv, in=4096, out=1024
linear: layers.6.attention.wo, in=4096, out=4096
linear: layers.6.feed_forward.w1, in=4096, out=14336
linear: layers.6.feed_forward.w2, in=14336, out=4096
linear: layers.6.feed_forward.w3, in=4096, out=14336
linear: layers.7.attention.wq, in=4096, out=4096
linear: layers.7.attention.wk, in=4096, out=1024
linear: layers.7.attention.wv, in=4096, out=1024
linear: layers.7.attention.wo, in=4096, out=4096
linear: layers.7.feed_forward.w1, in=4096, out=14336
linear: layers.7.feed_forward.w2, in=14336, out=4096
linear: layers.7.feed_forward.w3, in=4096, out=14336
linear: layers.8.attention.wq, in=4096, out=4096
linear: layers.8.attention.wk, in=4096, out=1024
linear: layers.8.attention.wv, in=4096, out=1024
linear: layers.8.attention.wo, in=4096, out=4096
linear: layers.8.feed_forward.w1, in=4096, out=14336
linear: layers.8.feed_forward.w2, in=14336, out=4096
linear: layers.8.feed_forward.w3, in=4096, out=14336
linear: layers.9.attention.wq, in=4096, out=4096
linear: layers.9.attention.wk, in=4096, out=1024
linear: layers.9.attention.wv, in=4096, out=1024
linear: layers.9.attention.wo, in=4096, out=4096
linear: layers.9.feed_forward.w1, in=4096, out=14336
linear: layers.9.feed_forward.w2, in=14336, out=4096
linear: layers.9.feed_forward.w3, in=4096, out=14336
linear: layers.10.attention.wq, in=4096, out=4096
linear: layers.10.attention.wk, in=4096, out=1024
linear: layers.10.attention.wv, in=4096, out=1024
linear: layers.10.attention.wo, in=4096, out=4096
linear: layers.10.feed_forward.w1, in=4096, out=14336
linear: layers.10.feed_forward.w2, in=14336, out=4096
linear: layers.10.feed_forward.w3, in=4096, out=14336
linear: layers.11.attention.wq, in=4096, out=4096
linear: layers.11.attention.wk, in=4096, out=1024
linear: layers.11.attention.wv, in=4096, out=1024
linear: layers.11.attention.wo, in=4096, out=4096
linear: layers.11.feed_forward.w1, in=4096, out=14336
linear: layers.11.feed_forward.w2, in=14336, out=4096
linear: layers.11.feed_forward.w3, in=4096, out=14336
linear: layers.12.attention.wq, in=4096, out=4096
linear: layers.12.attention.wk, in=4096, out=1024
linear: layers.12.attention.wv, in=4096, out=1024
linear: layers.12.attention.wo, in=4096, out=4096
linear: layers.12.feed_forward.w1, in=4096, out=14336
linear: layers.12.feed_forward.w2, in=14336, out=4096
linear: layers.12.feed_forward.w3, in=4096, out=14336
linear: layers.13.attention.wq, in=4096, out=4096
linear: layers.13.attention.wk, in=4096, out=1024
linear: layers.13.attention.wv, in=4096, out=1024
linear: layers.13.attention.wo, in=4096, out=4096
linear: layers.13.feed_forward.w1, in=4096, out=14336
linear: layers.13.feed_forward.w2, in=14336, out=4096
linear: layers.13.feed_forward.w3, in=4096, out=14336
linear: layers.14.attention.wq, in=4096, out=4096
linear: layers.14.attention.wk, in=4096, out=1024
linear: layers.14.attention.wv, in=4096, out=1024
linear: layers.14.attention.wo, in=4096, out=4096
linear: layers.14.feed_forward.w1, in=4096, out=14336
linear: layers.14.feed_forward.w2, in=14336, out=4096
linear: layers.14.feed_forward.w3, in=4096, out=14336
linear: layers.15.attention.wq, in=4096, out=4096
linear: layers.15.attention.wk, in=4096, out=1024
linear: layers.15.attention.wv, in=4096, out=1024
linear: layers.15.attention.wo, in=4096, out=4096
linear: layers.15.feed_forward.w1, in=4096, out=14336
linear: layers.15.feed_forward.w2, in=14336, out=4096
linear: layers.15.feed_forward.w3, in=4096, out=14336
linear: layers.16.attention.wq, in=4096, out=4096
linear: layers.16.attention.wk, in=4096, out=1024
linear: layers.16.attention.wv, in=4096, out=1024
linear: layers.16.attention.wo, in=4096, out=4096
linear: layers.16.feed_forward.w1, in=4096, out=14336
linear: layers.16.feed_forward.w2, in=14336, out=4096
linear: layers.16.feed_forward.w3, in=4096, out=14336
linear: layers.17.attention.wq, in=4096, out=4096
linear: layers.17.attention.wk, in=4096, out=1024
linear: layers.17.attention.wv, in=4096, out=1024
linear: layers.17.attention.wo, in=4096, out=4096
linear: layers.17.feed_forward.w1, in=4096, out=14336
linear: layers.17.feed_forward.w2, in=14336, out=4096
linear: layers.17.feed_forward.w3, in=4096, out=14336
linear: layers.18.attention.wq, in=4096, out=4096
linear: layers.18.attention.wk, in=4096, out=1024
linear: layers.18.attention.wv, in=4096, out=1024
linear: layers.18.attention.wo, in=4096, out=4096
linear: layers.18.feed_forward.w1, in=4096, out=14336
linear: layers.18.feed_forward.w2, in=14336, out=4096
linear: layers.18.feed_forward.w3, in=4096, out=14336
linear: layers.19.attention.wq, in=4096, out=4096
linear: layers.19.attention.wk, in=4096, out=1024
linear: layers.19.attention.wv, in=4096, out=1024
linear: layers.19.attention.wo, in=4096, out=4096
linear: layers.19.feed_forward.w1, in=4096, out=14336
linear: layers.19.feed_forward.w2, in=14336, out=4096
linear: layers.19.feed_forward.w3, in=4096, out=14336
linear: layers.20.attention.wq, in=4096, out=4096
linear: layers.20.attention.wk, in=4096, out=1024
linear: layers.20.attention.wv, in=4096, out=1024
linear: layers.20.attention.wo, in=4096, out=4096
linear: layers.20.feed_forward.w1, in=4096, out=14336
linear: layers.20.feed_forward.w2, in=14336, out=4096
linear: layers.20.feed_forward.w3, in=4096, out=14336
linear: layers.21.attention.wq, in=4096, out=4096
linear: layers.21.attention.wk, in=4096, out=1024
linear: layers.21.attention.wv, in=4096, out=1024
linear: layers.21.attention.wo, in=4096, out=4096
linear: layers.21.feed_forward.w1, in=4096, out=14336
linear: layers.21.feed_forward.w2, in=14336, out=4096
linear: layers.21.feed_forward.w3, in=4096, out=14336
linear: layers.22.attention.wq, in=4096, out=4096
linear: layers.22.attention.wk, in=4096, out=1024
linear: layers.22.attention.wv, in=4096, out=1024
linear: layers.22.attention.wo, in=4096, out=4096
linear: layers.22.feed_forward.w1, in=4096, out=14336
linear: layers.22.feed_forward.w2, in=14336, out=4096
linear: layers.22.feed_forward.w3, in=4096, out=14336
linear: layers.23.attention.wq, in=4096, out=4096
linear: layers.23.attention.wk, in=4096, out=1024
linear: layers.23.attention.wv, in=4096, out=1024
linear: layers.23.attention.wo, in=4096, out=4096
linear: layers.23.feed_forward.w1, in=4096, out=14336
linear: layers.23.feed_forward.w2, in=14336, out=4096
linear: layers.23.feed_forward.w3, in=4096, out=14336
linear: layers.24.attention.wq, in=4096, out=4096
linear: layers.24.attention.wk, in=4096, out=1024
linear: layers.24.attention.wv, in=4096, out=1024
linear: layers.24.attention.wo, in=4096, out=4096
linear: layers.24.feed_forward.w1, in=4096, out=14336
linear: layers.24.feed_forward.w2, in=14336, out=4096
linear: layers.24.feed_forward.w3, in=4096, out=14336
linear: layers.25.attention.wq, in=4096, out=4096
linear: layers.25.attention.wk, in=4096, out=1024
linear: layers.25.attention.wv, in=4096, out=1024
linear: layers.25.attention.wo, in=4096, out=4096
linear: layers.25.feed_forward.w1, in=4096, out=14336
linear: layers.25.feed_forward.w2, in=14336, out=4096
linear: layers.25.feed_forward.w3, in=4096, out=14336
linear: layers.26.attention.wq, in=4096, out=4096
linear: layers.26.attention.wk, in=4096, out=1024
linear: layers.26.attention.wv, in=4096, out=1024
linear: layers.26.attention.wo, in=4096, out=4096
linear: layers.26.feed_forward.w1, in=4096, out=14336
linear: layers.26.feed_forward.w2, in=14336, out=4096
linear: layers.26.feed_forward.w3, in=4096, out=14336
linear: layers.27.attention.wq, in=4096, out=4096
linear: layers.27.attention.wk, in=4096, out=1024
linear: layers.27.attention.wv, in=4096, out=1024
linear: layers.27.attention.wo, in=4096, out=4096
linear: layers.27.feed_forward.w1, in=4096, out=14336
linear: layers.27.feed_forward.w2, in=14336, out=4096
linear: layers.27.feed_forward.w3, in=4096, out=14336
linear: layers.28.attention.wq, in=4096, out=4096
linear: layers.28.attention.wk, in=4096, out=1024
linear: layers.28.attention.wv, in=4096, out=1024
linear: layers.28.attention.wo, in=4096, out=4096
linear: layers.28.feed_forward.w1, in=4096, out=14336
linear: layers.28.feed_forward.w2, in=14336, out=4096
linear: layers.28.feed_forward.w3, in=4096, out=14336
linear: layers.29.attention.wq, in=4096, out=4096
linear: layers.29.attention.wk, in=4096, out=1024
linear: layers.29.attention.wv, in=4096, out=1024
linear: layers.29.attention.wo, in=4096, out=4096
linear: layers.29.feed_forward.w1, in=4096, out=14336
linear: layers.29.feed_forward.w2, in=14336, out=4096
linear: layers.29.feed_forward.w3, in=4096, out=14336
linear: layers.30.attention.wq, in=4096, out=4096
linear: layers.30.attention.wk, in=4096, out=1024
linear: layers.30.attention.wv, in=4096, out=1024
linear: layers.30.attention.wo, in=4096, out=4096
linear: layers.30.feed_forward.w1, in=4096, out=14336
linear: layers.30.feed_forward.w2, in=14336, out=4096
linear: layers.30.feed_forward.w3, in=4096, out=14336
linear: layers.31.attention.wq, in=4096, out=4096
linear: layers.31.attention.wk, in=4096, out=1024
linear: layers.31.attention.wv, in=4096, out=1024
linear: layers.31.attention.wo, in=4096, out=4096
linear: layers.31.feed_forward.w1, in=4096, out=14336
linear: layers.31.feed_forward.w2, in=14336, out=4096
linear: layers.31.feed_forward.w3, in=4096, out=14336
linear: output, in=4096, out=128256
Using device=cpu Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz
Loading model...
Time to load model: 0.39 seconds
Quantizing the model with: {'linear:int4': {'groupsize': 256}, 'precision': {'dtype': 'bfloat16'}, 'executor': {'accelerator': 'cpu'}}
Time to quantize model: 34.03 seconds
-----------------------------------------------------------
Once upon a time, when I was a student at the University of Manchester, I had the chance to attend a lecture on "Women and the Media" given by a visiting professor from the American University of Beirut. The talk was enlightening, but I was astonished by the lack of interest from people around me. I would have expected many students to attend such a talk, especially at a time when the women's movement was gaining momentum.
Next time, I'm going to put up a PowerPoint presentation -- no, not a lecture, a PowerPoint presentation -- on the impact of globalization on the local economy. I'm going to use statistics, we'll talk about the welfare of the poor, and how the nation has been doing since the global economic crisis started. I'm thinking of inviting a representative from the local trade union, a bank representative, and maybe a big business owner to share their opinions. We'll have a good discussion where people will be able to ask questions and share their own perspectives.
Unfortunately, a major slide presentation can be a very traditional and dry way of presenting information. It's not as engaging as a talk or a panel discussion, for example.
You could think about using social media channels to share your ideas, maybe even running a blog or a podcast. That might be more
Time for inference 1: 86.02 sec total, time to first token 0.60 sec with parallel prefill, 255 tokens, 2.96 tokens/sec, 337.34 ms/token
Bandwidth achieved: 14.59 GB/s
*** This first iteration will include cold start effects for dynamic import, hardware caches. ***
Once upon a time, in a far-off land, there was a kingdom ruled by a wise and just king. The king had a beautiful daughter, Princess Sofia, who was loved by all the people in the kingdom. She had long golden hair and sparkling blue eyes that shone like the stars in the night sky. Princess Sofia was kind and gentle, and everyone in the kingdom adored her.
One day, a wicked sorcerer named Malakai appeared in the kingdom. He was a dark and evil man who had been banished from many lands for his cruel deeds. Malakai had a deep hatred for Princess Sofia and sought to use his magic to destroy her and take over the kingdom for himself.
The king, knowing of Malakai's evil intentions, called upon his bravest knights to protect his daughter from harm. The knights, led by the chivalrous Sir Edward, set out to ward off Malakai's dark magic and keep the princess safe.
Meanwhile, Princess Sofia was unaware of the danger that loomed over her. She spent her days playing with her friends and exploring the castle gardens, completely carefree and joyful.
But Malakai's magic was spreading fast, and soon the kingdom was shrouded in darkness. Crops began to wither and
Time for inference 2: 80.20 sec total, time to first token 0.74 sec with parallel prefill, 255 tokens, 3.18 tokens/sec, 314.51 ms/token
Bandwidth achieved: 15.65 GB/s
Once upon a time, there was a young girl named Lily who lived in a small village surrounded by a beautiful forest. Lily was known as the best singer in the village, and everyone would gather around her to listen to her sweet voice whenever she sang.
One day, a famous music teacher named Mr. Thompson arrived in the village to give a lecture on music and art. Mr. Thompson was known for his exceptional ability to recognize and develop talent in young people. He was on a mission to find a student who could bring out the best in him and learn from him.
As soon as Mr. Thompson arrived in the village, Lily heard about his arrival from her friends. Lily was determined to meet Mr. Thompson and showcase her singing talent to him. She decided to sneak away from her chores and visit Mr. Thompson's house, where he was staying temporarily. She snuck in through the back door and made her way to the room where she guessed Mr. Thompson would be resting.
To Lily's surprise, she saw Mr. Thompson playing the piano, and he was not happy to see her there. He told Lily that she needed to go back to her village and not disturb him while he was resting. Lily was heartbroken but didn't give up. She asked Mr. Thompson if he
Time for inference 3: 65.13 sec total, time to first token 0.32 sec with parallel prefill, 255 tokens, 3.92 tokens/sec, 255.40 ms/token
Bandwidth achieved: 19.27 GB/s

========================================

Average tokens/sec: 3.35
Memory used: 0.00 GB

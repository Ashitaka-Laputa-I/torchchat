python3 torchchat.py export llama3 --quantize '{"linear:int4": {"groupsize": 256}, "precision": {"dtype":"bfloat16"}, "executor":{"accelerator":"cpu"}}' --output-dso-path /tmp/model34.so
python3 torchchat.py generate llama3 --dso-path /tmp/model34.so --prompt "Once upon a time," --max-new-tokens 256 --device cpu --num-samples 3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
python3 torchchat.py export llama3 --quantize '{"linear:int4": {"groupsize": 256}, "precision": {"dtype":"bfloat16"}, "executor":{"accelerator":"cpu"}}' --output-dso-path /tmp/model34.so
Note: NumExpr detected 22 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
NumExpr defaulting to 16 threads.
PyTorch version 2.5.0.dev20240728+cu121 available.
linear: layers.0.attention.wq, in=4096, out=4096
linear: layers.0.attention.wk, in=4096, out=1024
linear: layers.0.attention.wv, in=4096, out=1024
linear: layers.0.attention.wo, in=4096, out=4096
linear: layers.0.feed_forward.w1, in=4096, out=14336
linear: layers.0.feed_forward.w2, in=14336, out=4096
linear: layers.0.feed_forward.w3, in=4096, out=14336
linear: layers.1.attention.wq, in=4096, out=4096
linear: layers.1.attention.wk, in=4096, out=1024
linear: layers.1.attention.wv, in=4096, out=1024
linear: layers.1.attention.wo, in=4096, out=4096
linear: layers.1.feed_forward.w1, in=4096, out=14336
linear: layers.1.feed_forward.w2, in=14336, out=4096
linear: layers.1.feed_forward.w3, in=4096, out=14336
linear: layers.2.attention.wq, in=4096, out=4096
linear: layers.2.attention.wk, in=4096, out=1024
linear: layers.2.attention.wv, in=4096, out=1024
linear: layers.2.attention.wo, in=4096, out=4096
linear: layers.2.feed_forward.w1, in=4096, out=14336
linear: layers.2.feed_forward.w2, in=14336, out=4096
linear: layers.2.feed_forward.w3, in=4096, out=14336
linear: layers.3.attention.wq, in=4096, out=4096
linear: layers.3.attention.wk, in=4096, out=1024
linear: layers.3.attention.wv, in=4096, out=1024
linear: layers.3.attention.wo, in=4096, out=4096
linear: layers.3.feed_forward.w1, in=4096, out=14336
linear: layers.3.feed_forward.w2, in=14336, out=4096
linear: layers.3.feed_forward.w3, in=4096, out=14336
linear: layers.4.attention.wq, in=4096, out=4096
linear: layers.4.attention.wk, in=4096, out=1024
linear: layers.4.attention.wv, in=4096, out=1024
linear: layers.4.attention.wo, in=4096, out=4096
linear: layers.4.feed_forward.w1, in=4096, out=14336
linear: layers.4.feed_forward.w2, in=14336, out=4096
linear: layers.4.feed_forward.w3, in=4096, out=14336
linear: layers.5.attention.wq, in=4096, out=4096
linear: layers.5.attention.wk, in=4096, out=1024
linear: layers.5.attention.wv, in=4096, out=1024
linear: layers.5.attention.wo, in=4096, out=4096
linear: layers.5.feed_forward.w1, in=4096, out=14336
linear: layers.5.feed_forward.w2, in=14336, out=4096
linear: layers.5.feed_forward.w3, in=4096, out=14336
linear: layers.6.attention.wq, in=4096, out=4096
linear: layers.6.attention.wk, in=4096, out=1024
linear: layers.6.attention.wv, in=4096, out=1024
linear: layers.6.attention.wo, in=4096, out=4096
linear: layers.6.feed_forward.w1, in=4096, out=14336
linear: layers.6.feed_forward.w2, in=14336, out=4096
linear: layers.6.feed_forward.w3, in=4096, out=14336
linear: layers.7.attention.wq, in=4096, out=4096
linear: layers.7.attention.wk, in=4096, out=1024
linear: layers.7.attention.wv, in=4096, out=1024
linear: layers.7.attention.wo, in=4096, out=4096
linear: layers.7.feed_forward.w1, in=4096, out=14336
linear: layers.7.feed_forward.w2, in=14336, out=4096
linear: layers.7.feed_forward.w3, in=4096, out=14336
linear: layers.8.attention.wq, in=4096, out=4096
linear: layers.8.attention.wk, in=4096, out=1024
linear: layers.8.attention.wv, in=4096, out=1024
linear: layers.8.attention.wo, in=4096, out=4096
linear: layers.8.feed_forward.w1, in=4096, out=14336
linear: layers.8.feed_forward.w2, in=14336, out=4096
linear: layers.8.feed_forward.w3, in=4096, out=14336
linear: layers.9.attention.wq, in=4096, out=4096
linear: layers.9.attention.wk, in=4096, out=1024
linear: layers.9.attention.wv, in=4096, out=1024
linear: layers.9.attention.wo, in=4096, out=4096
linear: layers.9.feed_forward.w1, in=4096, out=14336
linear: layers.9.feed_forward.w2, in=14336, out=4096
linear: layers.9.feed_forward.w3, in=4096, out=14336
linear: layers.10.attention.wq, in=4096, out=4096
linear: layers.10.attention.wk, in=4096, out=1024
linear: layers.10.attention.wv, in=4096, out=1024
linear: layers.10.attention.wo, in=4096, out=4096
linear: layers.10.feed_forward.w1, in=4096, out=14336
linear: layers.10.feed_forward.w2, in=14336, out=4096
linear: layers.10.feed_forward.w3, in=4096, out=14336
linear: layers.11.attention.wq, in=4096, out=4096
linear: layers.11.attention.wk, in=4096, out=1024
linear: layers.11.attention.wv, in=4096, out=1024
linear: layers.11.attention.wo, in=4096, out=4096
linear: layers.11.feed_forward.w1, in=4096, out=14336
linear: layers.11.feed_forward.w2, in=14336, out=4096
linear: layers.11.feed_forward.w3, in=4096, out=14336
linear: layers.12.attention.wq, in=4096, out=4096
linear: layers.12.attention.wk, in=4096, out=1024
linear: layers.12.attention.wv, in=4096, out=1024
linear: layers.12.attention.wo, in=4096, out=4096
linear: layers.12.feed_forward.w1, in=4096, out=14336
linear: layers.12.feed_forward.w2, in=14336, out=4096
linear: layers.12.feed_forward.w3, in=4096, out=14336
linear: layers.13.attention.wq, in=4096, out=4096
linear: layers.13.attention.wk, in=4096, out=1024
linear: layers.13.attention.wv, in=4096, out=1024
linear: layers.13.attention.wo, in=4096, out=4096
linear: layers.13.feed_forward.w1, in=4096, out=14336
linear: layers.13.feed_forward.w2, in=14336, out=4096
linear: layers.13.feed_forward.w3, in=4096, out=14336
linear: layers.14.attention.wq, in=4096, out=4096
linear: layers.14.attention.wk, in=4096, out=1024
linear: layers.14.attention.wv, in=4096, out=1024
linear: layers.14.attention.wo, in=4096, out=4096
linear: layers.14.feed_forward.w1, in=4096, out=14336
linear: layers.14.feed_forward.w2, in=14336, out=4096
linear: layers.14.feed_forward.w3, in=4096, out=14336
linear: layers.15.attention.wq, in=4096, out=4096
linear: layers.15.attention.wk, in=4096, out=1024
linear: layers.15.attention.wv, in=4096, out=1024
linear: layers.15.attention.wo, in=4096, out=4096
linear: layers.15.feed_forward.w1, in=4096, out=14336
linear: layers.15.feed_forward.w2, in=14336, out=4096
linear: layers.15.feed_forward.w3, in=4096, out=14336
linear: layers.16.attention.wq, in=4096, out=4096
linear: layers.16.attention.wk, in=4096, out=1024
linear: layers.16.attention.wv, in=4096, out=1024
linear: layers.16.attention.wo, in=4096, out=4096
linear: layers.16.feed_forward.w1, in=4096, out=14336
linear: layers.16.feed_forward.w2, in=14336, out=4096
linear: layers.16.feed_forward.w3, in=4096, out=14336
linear: layers.17.attention.wq, in=4096, out=4096
linear: layers.17.attention.wk, in=4096, out=1024
linear: layers.17.attention.wv, in=4096, out=1024
linear: layers.17.attention.wo, in=4096, out=4096
linear: layers.17.feed_forward.w1, in=4096, out=14336
linear: layers.17.feed_forward.w2, in=14336, out=4096
linear: layers.17.feed_forward.w3, in=4096, out=14336
linear: layers.18.attention.wq, in=4096, out=4096
linear: layers.18.attention.wk, in=4096, out=1024
linear: layers.18.attention.wv, in=4096, out=1024
linear: layers.18.attention.wo, in=4096, out=4096
linear: layers.18.feed_forward.w1, in=4096, out=14336
linear: layers.18.feed_forward.w2, in=14336, out=4096
linear: layers.18.feed_forward.w3, in=4096, out=14336
linear: layers.19.attention.wq, in=4096, out=4096
linear: layers.19.attention.wk, in=4096, out=1024
linear: layers.19.attention.wv, in=4096, out=1024
linear: layers.19.attention.wo, in=4096, out=4096
linear: layers.19.feed_forward.w1, in=4096, out=14336
linear: layers.19.feed_forward.w2, in=14336, out=4096
linear: layers.19.feed_forward.w3, in=4096, out=14336
linear: layers.20.attention.wq, in=4096, out=4096
linear: layers.20.attention.wk, in=4096, out=1024
linear: layers.20.attention.wv, in=4096, out=1024
linear: layers.20.attention.wo, in=4096, out=4096
linear: layers.20.feed_forward.w1, in=4096, out=14336
linear: layers.20.feed_forward.w2, in=14336, out=4096
linear: layers.20.feed_forward.w3, in=4096, out=14336
linear: layers.21.attention.wq, in=4096, out=4096
linear: layers.21.attention.wk, in=4096, out=1024
linear: layers.21.attention.wv, in=4096, out=1024
linear: layers.21.attention.wo, in=4096, out=4096
linear: layers.21.feed_forward.w1, in=4096, out=14336
linear: layers.21.feed_forward.w2, in=14336, out=4096
linear: layers.21.feed_forward.w3, in=4096, out=14336
linear: layers.22.attention.wq, in=4096, out=4096
linear: layers.22.attention.wk, in=4096, out=1024
linear: layers.22.attention.wv, in=4096, out=1024
linear: layers.22.attention.wo, in=4096, out=4096
linear: layers.22.feed_forward.w1, in=4096, out=14336
linear: layers.22.feed_forward.w2, in=14336, out=4096
linear: layers.22.feed_forward.w3, in=4096, out=14336
linear: layers.23.attention.wq, in=4096, out=4096
linear: layers.23.attention.wk, in=4096, out=1024
linear: layers.23.attention.wv, in=4096, out=1024
linear: layers.23.attention.wo, in=4096, out=4096
linear: layers.23.feed_forward.w1, in=4096, out=14336
linear: layers.23.feed_forward.w2, in=14336, out=4096
linear: layers.23.feed_forward.w3, in=4096, out=14336
linear: layers.24.attention.wq, in=4096, out=4096
linear: layers.24.attention.wk, in=4096, out=1024
linear: layers.24.attention.wv, in=4096, out=1024
linear: layers.24.attention.wo, in=4096, out=4096
linear: layers.24.feed_forward.w1, in=4096, out=14336
linear: layers.24.feed_forward.w2, in=14336, out=4096
linear: layers.24.feed_forward.w3, in=4096, out=14336
linear: layers.25.attention.wq, in=4096, out=4096
linear: layers.25.attention.wk, in=4096, out=1024
linear: layers.25.attention.wv, in=4096, out=1024
linear: layers.25.attention.wo, in=4096, out=4096
linear: layers.25.feed_forward.w1, in=4096, out=14336
linear: layers.25.feed_forward.w2, in=14336, out=4096
linear: layers.25.feed_forward.w3, in=4096, out=14336
linear: layers.26.attention.wq, in=4096, out=4096
linear: layers.26.attention.wk, in=4096, out=1024
linear: layers.26.attention.wv, in=4096, out=1024
linear: layers.26.attention.wo, in=4096, out=4096
linear: layers.26.feed_forward.w1, in=4096, out=14336
linear: layers.26.feed_forward.w2, in=14336, out=4096
linear: layers.26.feed_forward.w3, in=4096, out=14336
linear: layers.27.attention.wq, in=4096, out=4096
linear: layers.27.attention.wk, in=4096, out=1024
linear: layers.27.attention.wv, in=4096, out=1024
linear: layers.27.attention.wo, in=4096, out=4096
linear: layers.27.feed_forward.w1, in=4096, out=14336
linear: layers.27.feed_forward.w2, in=14336, out=4096
linear: layers.27.feed_forward.w3, in=4096, out=14336
linear: layers.28.attention.wq, in=4096, out=4096
linear: layers.28.attention.wk, in=4096, out=1024
linear: layers.28.attention.wv, in=4096, out=1024
linear: layers.28.attention.wo, in=4096, out=4096
linear: layers.28.feed_forward.w1, in=4096, out=14336
linear: layers.28.feed_forward.w2, in=14336, out=4096
linear: layers.28.feed_forward.w3, in=4096, out=14336
linear: layers.29.attention.wq, in=4096, out=4096
linear: layers.29.attention.wk, in=4096, out=1024
linear: layers.29.attention.wv, in=4096, out=1024
linear: layers.29.attention.wo, in=4096, out=4096
linear: layers.29.feed_forward.w1, in=4096, out=14336
linear: layers.29.feed_forward.w2, in=14336, out=4096
linear: layers.29.feed_forward.w3, in=4096, out=14336
linear: layers.30.attention.wq, in=4096, out=4096
linear: layers.30.attention.wk, in=4096, out=1024
linear: layers.30.attention.wv, in=4096, out=1024
linear: layers.30.attention.wo, in=4096, out=4096
linear: layers.30.feed_forward.w1, in=4096, out=14336
linear: layers.30.feed_forward.w2, in=14336, out=4096
linear: layers.30.feed_forward.w3, in=4096, out=14336
linear: layers.31.attention.wq, in=4096, out=4096
linear: layers.31.attention.wk, in=4096, out=1024
linear: layers.31.attention.wv, in=4096, out=1024
linear: layers.31.attention.wo, in=4096, out=4096
linear: layers.31.feed_forward.w1, in=4096, out=14336
linear: layers.31.feed_forward.w2, in=14336, out=4096
linear: layers.31.feed_forward.w3, in=4096, out=14336
linear: output, in=4096, out=128256
Using device=cpu
Setting max_seq_length to 300 for DSO export.
Loading model...
Time to load model: 0.39 seconds
Quantizing the model with: {'linear:int4': {'groupsize': 256}, 'precision': {'dtype': 'bfloat16'}, 'executor': {'accelerator': 'cpu'}}
Time to quantize model: 54.73 seconds
-----------------------------------------------------------
Exporting model using AOT Inductor to /tmp/model34.so
The generated DSO model can be found at: /tmp/model34.so
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
python3 torchchat.py generate llama3 --dso-path /tmp/model34.so --prompt "Once upon a time," --max-new-tokens 256 --device cpu --num-samples 3
Note: NumExpr detected 22 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
NumExpr defaulting to 16 threads.
PyTorch version 2.5.0.dev20240728+cu121 available.
Warning: checkpoint path ignored because an exported DSO or PTE path specified
Warning: checkpoint path ignored because an exported DSO or PTE path specified
Using device=cpu Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz
Loading model...
Time to load model: 0.38 seconds
-----------------------------------------------------------
Once upon a time, in a small village, there lived a young boy named Kaito. Kaito was an ordinary boy, with an ordinary life. He had a loving family, a small gang of friends, and a job at a local bakery. He worked there, along with his best friend, a girl named Akira, helping his uncle, who was the owner of the bakery.
One day, while Kaito and Akira were delivering bread to the villagers, they stumbled upon a mysterious, ancient-looking book buried deep in the earth. The book was bound in an unusual black leather, and it had an unusual symbol etched onto the cover.
As soon as they picked up the book, they felt an unusual energy emanating from it. The book began to glow, and the symbol on the cover started to change, revealing a hidden message.
Kaito and Akira were drawn to the book, as if it was calling them. They opened the book, and found that it was filled with strange, intricate pages. The pages were filled with diagrams, symbols, and strange, wispy writing.
As they began to flip through the pages, they noticed that the pages were turning on their own, revealing hidden sections and messages. They discovered that the book was an
Time for inference 1: 75.82 sec total, time to first token 2.70 sec with sequential prefill, 255 tokens, 3.36 tokens/sec, 297.34 ms/token
Bandwidth achieved: 54.01 GB/s
*** This first iteration will include cold start effects for dynamic import, hardware caches. ***
Once upon a time, there was a world where women wore kilts and men wore skirts. It was a world where social norms were turned upside down, and people lived according to their own rules. It was a world where people were free to be who they truly were, without fear of judgment or persecution.
This was the world that the people of Scotland lived in during the 18th century. During this time, Scotland was a place of great beauty and great turmoil. The country was ruled by the British, and the Scots were forced to wear kilts and tartan shirts, which were symbol of their nationality, but also of their subjugation.
The people of Scotland were known for their bravery and their love of freedom. They were a proud people, who were determined to keep their identity and their culture alive, even in the face of oppression.
One of the most famous Scottish heroes of this time was Rob Roy, a Scottish folk hero who lived in the 18th century. Rob Roy was a brave and a clever man, who was known for his cunning and his bravery. He was a natural leader, and he was loved by the people of Scotland for his bravery and his ability to inspire and to lead.
Rob Roy was also a man of great intelligence and charisma. He was
Time for inference 2: 88.38 sec total, time to first token 2.38 sec with sequential prefill, 255 tokens, 2.89 tokens/sec, 346.59 ms/token
Bandwidth achieved: 46.34 GB/s
Once upon a time, in a small village nestled in the rolling hills of the countryside, there lived a young girl named Sophia. Sophia was known throughout the village for her extraordinary talent in baking. Every morning, she would wake up before dawn to mix, knead, and bake the most delicious pastries, cakes, and breads. The aroma of freshly baked goods wafting from her small bakery would fill the village streets, enticing everyone to come and sample her creations.

Sophia's baking skills were honed by her mother, who had learned the secrets of traditional baking from her own mother and grandmother. It was as if the art of baking was passed down through the generations, and Sophia was now the latest guardian of this family tradition.

One day, as Sophia was preparing the day's batch of freshly baked bread, a group of strangers passed by her bakery. They were a peculiar group, with faces that seemed from another time. They moved quickly, their eyes scanning the streets, their voices speaking in hushed tones.

Sophia noticed that they seemed to be searching for something and decided to offer them some freshly baked bread to warm their spirits. As she handed them a warm baguette, she sensed that they were not like the villagers she knew. Their eyes seemed to hold a
Time for inference 3: 39.35 sec total, time to first token 1.90 sec with sequential prefill, 255 tokens, 6.48 tokens/sec, 154.30 ms/token
Bandwidth achieved: 104.09 GB/s

========================================

Average tokens/sec: 4.24
Memory used: 0.00 GB
